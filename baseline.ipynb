{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzH6-LrXlxWr",
        "outputId": "adcdf4f5-ccf2-4d89-fde4-1e1ebbd55f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Đọc dữ liệu từ file\n",
        "sentences = []\n",
        "labels = []\n",
        "with open('/content/drive/MyDrive/myDrive/train.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        if '\\t' not in line:\n",
        "            continue\n",
        "        label, sentence = line.strip().split('\\t')\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "# Tiền xử lý dữ liệu\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'\\W+', ' ', sentence)\n",
        "    words = sentence.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "sentences = [preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Chuyển đổi dữ liệu văn bản thành các đặc trưng số sử dụng Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "X = tokenizer.texts_to_sequences(sentences)\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "\n",
        "# Chia dữ liệu thành hai tập con\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Chuyển đổi nhãn thành ma trận one-hot encoding\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "y_train = to_categorical(y_train_enc)\n",
        "y_test = to_categorical(y_test_enc)\n",
        "\n",
        "# Xây dựng mô hình LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 128, input_length=X.shape[1]))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
        "\n",
        "# Đánh giá mô hình bằng độ đo F1-score\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "f1 = f1_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_Vt5n9rl1mt",
        "outputId": "77b2617d-a0aa-46b6-da25-e66a8cb5603e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 91s 697ms/step - loss: 1.0514 - accuracy: 0.5544 - val_loss: 0.8236 - val_accuracy: 0.6980\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 90s 721ms/step - loss: 0.8035 - accuracy: 0.6972 - val_loss: 0.7420 - val_accuracy: 0.7280\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 86s 688ms/step - loss: 0.7295 - accuracy: 0.7247 - val_loss: 0.7432 - val_accuracy: 0.7240\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 85s 678ms/step - loss: 0.6875 - accuracy: 0.7402 - val_loss: 0.7498 - val_accuracy: 0.7290\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 86s 689ms/step - loss: 0.6587 - accuracy: 0.7497 - val_loss: 0.7592 - val_accuracy: 0.7230\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 84s 667ms/step - loss: 0.6162 - accuracy: 0.7634 - val_loss: 0.7920 - val_accuracy: 0.7130\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 85s 679ms/step - loss: 0.5835 - accuracy: 0.7702 - val_loss: 0.8193 - val_accuracy: 0.7050\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 85s 682ms/step - loss: 0.5493 - accuracy: 0.7804 - val_loss: 0.8355 - val_accuracy: 0.7150\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 84s 672ms/step - loss: 0.5160 - accuracy: 0.7922 - val_loss: 0.8941 - val_accuracy: 0.7070\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 82s 660ms/step - loss: 0.5018 - accuracy: 0.7924 - val_loss: 0.9070 - val_accuracy: 0.7020\n",
            "32/32 [==============================] - 3s 70ms/step\n",
            "F1-score: 0.6649505579424531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data để pred\n",
        "phase_0=[]\n",
        "with open('/content/drive/MyDrive/myDrive/data_phase_1.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    phase_0.append(line)\n",
        "\n",
        "# Tiến hành model\n",
        "\n",
        "T = tokenizer.texts_to_sequences([preprocess(p) for p in phase_0])\n",
        "T = pad_sequences(T, maxlen=200)\n",
        "Pred = model.predict(T)\n",
        "Pred = np.argmax(Pred, axis=1)\n",
        "Pred = le.inverse_transform(Pred)\n",
        "print(Pred)\n",
        "\n",
        "# Trích xuất chỉ có label\n",
        "with open(\"/content/drive/MyDrive/myDrive/output_with_labels.txt\", \"w\") as f:\n",
        "    for line in Pred:\n",
        "      f.write(line+'\\n')\n"
      ],
      "metadata": {
        "id": "wWfIGHtJl1qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22249f7-99d9-4db4-dc99-ad87a7a72573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 75ms/step\n",
            "['negative' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
            " 'positive' 'neutral    ' 'positive' 'positive' 'positive' 'negative'\n",
            " 'negative' 'negative' 'positive' 'neutral    ' 'negative' 'negative'\n",
            " 'negative' 'positive' 'neutral    ' 'positive' 'neutral    ' 'negative'\n",
            " 'negative' 'neutral    ' 'neutral    ' 'negative' 'negative' 'positive'\n",
            " 'neutral    ' 'positive' 'negative' 'positive' 'positive' 'neutral    '\n",
            " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
            " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
            " 'negative' 'positive' 'neutral    ' 'neutral    ' 'positive' 'positive'\n",
            " 'negative' 'negative' 'neutral    ' 'negative' 'negative' 'neutral    '\n",
            " 'negative' 'positive' 'neutral    ' 'negative' 'neutral    ' 'negative'\n",
            " 'negative' 'positive' 'negative' 'negative' 'neutral    ' 'neutral    '\n",
            " 'positive' 'positive' 'negative' 'neutral    ' 'negative' 'positive'\n",
            " 'negative' 'positive' 'positive' 'negative' 'neutral    ' 'positive'\n",
            " 'neutral    ' 'neutral    ' 'positive' 'negative' 'neutral    '\n",
            " 'negative' 'negative' 'positive' 'negative' 'neutral    ' 'negative'\n",
            " 'positive' 'negative' 'neutral    ' 'positive' 'negative' 'negative'\n",
            " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
            " 'negative' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
            " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
            " 'neutral    ' 'negative' 'positive' 'negative' 'neutral    ' 'positive'\n",
            " 'neutral    ' 'negative' 'negative' 'neutral    ' 'positive' 'positive'\n",
            " 'negative' 'positive' 'negative' 'neutral    ' 'negative' 'neutral    '\n",
            " 'positive' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
            " 'neutral    ' 'positive' 'neutral    ' 'negative' 'positive'\n",
            " 'neutral    ' 'neutral    ' 'negative' 'negative' 'negative' 'negative'\n",
            " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
            " 'negative' 'neutral    ' 'positive' 'positive' 'neutral    ' 'negative'\n",
            " 'neutral    ' 'negative' 'positive' 'neutral    ' 'positive'\n",
            " 'neutral    ' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
            " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
            " 'positive' 'negative' 'negative' 'negative' 'neutral    ' 'neutral    '\n",
            " 'negative' 'positive' 'positive' 'negative' 'negative' 'negative'\n",
            " 'negative' 'positive' 'negative' 'positive' 'positive' 'negative'\n",
            " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
            " 'positive' 'neutral    ' 'positive' 'positive' 'positive' 'negative'\n",
            " 'positive' 'positive' 'negative' 'negative' 'neutral    ' 'positive'\n",
            " 'positive' 'negative' 'neutral    ' 'negative' 'positive' 'negative'\n",
            " 'negative' 'neutral    ' 'neutral    ' 'negative' 'positive' 'negative'\n",
            " 'negative' 'positive' 'negative' 'neutral    ' 'negative' 'negative'\n",
            " 'negative' 'neutral    ' 'negative' 'negative' 'positive' 'negative'\n",
            " 'negative' 'neutral    ' 'negative' 'positive' 'negative' 'positive'\n",
            " 'positive' 'positive' 'neutral    ' 'neutral    ' 'negative' 'negative'\n",
            " 'negative' 'neutral    ' 'neutral    ' 'negative' 'negative' 'negative'\n",
            " 'positive' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
            " 'negative' 'negative' 'negative' 'negative' 'negative' 'neutral    '\n",
            " 'negative' 'positive' 'negative' 'negative' 'negative' 'positive'\n",
            " 'negative' 'negative' 'positive' 'positive' 'negative' 'neutral    '\n",
            " 'positive' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
            " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
            " 'positive' 'positive' 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ht6SVMvqnglJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}